{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.set_random_seed(1)\n",
    "import cv2\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, merge\n",
    "from keras.optimizers import Adam\n",
    "from fr_utils import *\n",
    "from inception_blocks_v2 import *\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ALPHA = 0.2  # Triplet Loss Parameter\n",
    "\n",
    "# Source: https://github.com/davidsandberg/facenet/blob/master/src/facenet.py\n",
    "def triplet_loss(x):\n",
    "    anchor, positive, negative = x\n",
    "\n",
    "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), 1)\n",
    "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), 1)\n",
    "\n",
    "    basic_loss = tf.add(tf.subtract(pos_dist, neg_dist), ALPHA)\n",
    "    loss = tf.reduce_mean(tf.maximum(basic_loss, 0.0), 0)\n",
    "\n",
    "    return loss\n",
    "\n",
    "# Builds an embedding for each example (i.e., positive, negative, anchor)\n",
    "# Then calculates the triplet loss between their embedding.\n",
    "# Then applies identity loss on the triplet loss value to minimize it on training.\n",
    "def build_model(input_shape):\n",
    "    # Standardizing the input shape order\n",
    "    K.set_image_dim_ordering('th')\n",
    "\n",
    "    positive_example = Input(shape=input_shape)\n",
    "    negative_example = Input(shape=input_shape)\n",
    "    anchor_example = Input(shape=input_shape)\n",
    "\n",
    "    # Create Common network to share the weights along different examples (+/-/Anchor)\n",
    "    embedding_network = faceRecoModel(input_shape)\n",
    "\n",
    "    positive_embedding = embedding_network(positive_example)\n",
    "    negative_embedding = embedding_network(negative_example)\n",
    "    anchor_embedding = embedding_network(anchor_example)\n",
    "\n",
    "    loss = merge([anchor_embedding, positive_embedding, negative_embedding],\n",
    "                 mode=triplet_loss, output_shape=(1,))\n",
    "    model = Model(inputs=[anchor_example, positive_example, negative_example],\n",
    "                  outputs=loss)\n",
    "    model.compile(loss='mean_absolute_error', optimizer=Adam())\n",
    "\n",
    "    return model\n",
    "\n",
    "# When fitting the model (i.e., model.fit()); use as an input [anchor_example,\n",
    "# positive_example, negative_example] in that order and as an output zero.\n",
    "# The reason to use the output as zero is that you are trying to minimize the \n",
    "# triplet loss as much as possible and the minimum value of the loss is zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:36: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/usr/local/lib/python3.5/dist-packages/keras/legacy/layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    }
   ],
   "source": [
    "fmodel = build_model(input_shape=(3, 96, 96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## sample dataset\n",
    "# m = 5\n",
    "# x  = []\n",
    "# y = []\n",
    "# for i in range(m):\n",
    "#     a = np.random.random( (1, 3, 96, 96) )\n",
    "#     b = np.random.random( (1, 3, 96, 96) )\n",
    "#     c = np.random.random( (1, 3, 96, 96) )\n",
    "#     x.append([a,b,c])\n",
    "    \n",
    "#     y.append(np.zeros((1,1)) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_data = np.load('mnist_triplet_dataset.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_img(a_img, p_img, n_img):\n",
    "    a_img = cv2.cvtColor(a_img,cv2.COLOR_GRAY2RGB) \n",
    "    p_img= cv2.cvtColor(p_img,cv2.COLOR_GRAY2RGB)\n",
    "    n_img = cv2.cvtColor(n_img,cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    a_img = cv2.resize(a_img, (96, 96)) / 255.0\n",
    "    p_img = cv2.resize(p_img, (96, 96)) / 255.0\n",
    "    n_img = cv2.resize(n_img, (96, 96)) / 255.0\n",
    "\n",
    "\n",
    "    a_img = np.rollaxis(a_img, 2, 0)\n",
    "    a_img = a_img.reshape((1, a_img.shape[0],  a_img.shape[1],  a_img.shape[2]))\n",
    "\n",
    "    p_img = np.rollaxis(p_img, 2, 0)\n",
    "    p_img = p_img.reshape((1, p_img.shape[0],  p_img.shape[1],  p_img.shape[2]))\n",
    "\n",
    "    n_img = np.rollaxis(n_img, 2, 0)\n",
    "    n_img = n_img.reshape((1, n_img.shape[0],  n_img.shape[1],  n_img.shape[2]))\n",
    "    \n",
    "    return a_img, p_img, n_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [1:01:02<00:00, 13.65it/s]\n"
     ]
    }
   ],
   "source": [
    "for x in tqdm(x_data):\n",
    "    a_img, p_img, n_img = process_img(x[0], x[1], x[2])\n",
    "    \n",
    "    verbose = 1 if i % 1000 == 0 else 0        \n",
    "    fmodel.fit([a_img, p_img, n_img], np.zeros((1,1)), verbose=0)\n",
    "    \n",
    "#     if verbose == 1:\n",
    "#         print('\\nsteps ', i)\n",
    "    \n",
    "fmodel.save('mnist_weight.h5')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_3 (InputLayer)             (None, 3, 96, 96)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_1 (InputLayer)             (None, 3, 96, 96)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 3, 96, 96)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "FaceRecoModel (Model)            (None, 128)           3743280     input_1[0][0]                    \n",
      "                                                                   input_2[0][0]                    \n",
      "                                                                   input_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "merge_1 (Merge)                  (None, 1)             0           FaceRecoModel[3][0]              \n",
      "                                                                   FaceRecoModel[1][0]              \n",
      "                                                                   FaceRecoModel[2][0]              \n",
      "====================================================================================================\n",
      "Total params: 3,743,280\n",
      "Trainable params: 3,733,968\n",
      "Non-trainable params: 9,312\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "fmodel.summary()\n",
    "\n",
    "# from keras.utils import plot_model\n",
    "# plot_model(fmodel, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 3, 96, 96)         0         \n",
      "_________________________________________________________________\n",
      "FaceRecoModel (Model)        (None, 128)               3743280   \n",
      "=================================================================\n",
      "Total params: 3,743,280\n",
      "Trainable params: 3,733,968\n",
      "Non-trainable params: 9,312\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_new = Model(inputs=fmodel.layers[3].get_input_at(-1), \n",
    "                  outputs=fmodel.layers[3].get_output_at(-1))\n",
    "print(model_new.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_new.save('Siamese_Networks_weight.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(_, _), (test_x, test_y) = mnist.load_data()\n",
    "\n",
    "test_x = test_x[0:2000]\n",
    "test_y = test_y[0:2000]\n",
    "\n",
    "emb_tsv_str = '' \n",
    "emb_meta_str = ''\n",
    "\n",
    "for index, img in enumerate (test_x):\n",
    "    \n",
    "    img, _, _ = process_img(img, img, img)\n",
    "    \n",
    "    res = model_new.predict(img)\n",
    "    \n",
    "    res_str ='\\t'.join ([str(x) for x in res.tolist()[0]])\n",
    "    emb_tsv_str += res_str + '\\n'\n",
    "    \n",
    "    emb_meta_str += str(test_y[index]) + '\\n'\n",
    "    \n",
    "    \n",
    "with open('embd_mnist.tsv', 'w') as f:\n",
    "    f.write(emb_tsv_str)\n",
    "    \n",
    "with open('emb_meta_str.tsv', 'w') as f:\n",
    "    f.write(emb_meta_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
